// BlackRoad LLM API Gateway
// Cloudflare Worker - proxies to Pi cluster via Cloudflare Tunnel
// Agent: Icarus (b3e01bd9)

const CLUSTER_ENDPOINT = 'https://llm.blackroad.io'; // Via Cloudflare Tunnel

export default {
  async fetch(request, env, ctx) {
    const url = new URL(request.url);
    const path = url.pathname;

    // CORS headers
    const corsHeaders = {
      'Access-Control-Allow-Origin': '*',
      'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
      'Access-Control-Allow-Headers': 'Content-Type',
    };

    if (request.method === 'OPTIONS') {
      return new Response(null, { headers: corsHeaders });
    }

    // Routes
    if (path === '/api/llm/status') {
      return handleStatus(corsHeaders);
    }

    if (path === '/api/llm/generate' && request.method === 'POST') {
      return handleGenerate(request, corsHeaders);
    }

    if (path === '/api/llm/chat' && request.method === 'POST') {
      return handleChat(request, corsHeaders);
    }

    if (path === '/api/llm/models') {
      return handleModels(corsHeaders);
    }

    // Default: API docs
    return new Response(JSON.stringify({
      name: 'BlackRoad LLM API',
      version: '1.0.0',
      endpoints: {
        'GET /api/llm/status': 'Cluster status',
        'GET /api/llm/models': 'Available models',
        'POST /api/llm/generate': 'Generate text',
        'POST /api/llm/chat': 'Chat completion',
      },
      cluster: {
        nodes: ['lucidia', 'cecilia', 'octavia', 'aria'],
        models: 20,
        hailo_tops: 52,
      }
    }, null, 2), {
      headers: { ...corsHeaders, 'Content-Type': 'application/json' }
    });
  }
};

async function handleStatus(corsHeaders) {
  // In production, this would query the actual cluster
  const status = {
    status: 'healthy',
    nodes: {
      lucidia: { status: 'online', models: 6 },
      cecilia: { status: 'online', models: 6, hailo: true },
      octavia: { status: 'online', models: 7 },
      aria: { status: 'online', models: 1 },
    },
    totals: {
      nodes_online: 4,
      models: 20,
      hailo_tops: 52,
      containers: 160,
    },
    performance: {
      llm_tokens_per_sec: 5.7,
      llm_requests_per_sec: 3.33,
      hailo_fps: 296,
      hailo_watts: 4.6,
    }
  };

  return new Response(JSON.stringify(status, null, 2), {
    headers: { ...corsHeaders, 'Content-Type': 'application/json' }
  });
}

async function handleModels(corsHeaders) {
  const models = {
    llm: [
      { name: 'llama3.2:1b', nodes: ['lucidia', 'cecilia', 'octavia'], size: '1.3GB' },
      { name: 'llama3.2:3b', nodes: ['lucidia', 'cecilia'], size: '2.0GB' },
      { name: 'codellama:7b', nodes: ['lucidia', 'cecilia', 'octavia'], size: '3.8GB' },
      { name: 'tinyllama', nodes: ['lucidia', 'cecilia', 'octavia'], size: '637MB' },
      { name: 'phi3.5', nodes: ['lucidia'], size: '2.2GB' },
      { name: 'gemma2:2b', nodes: ['lucidia'], size: '1.6GB' },
      { name: 'qwen2.5:1.5b', nodes: ['octavia'], size: '1.1GB' },
      { name: 'qwen2.5-coder:3b', nodes: ['aria'], size: '1.9GB' },
    ],
    vision: [
      { name: 'yolov8s', accelerator: 'hailo-8', fps: 148, watts: 2.3 },
      { name: 'yolov6n', accelerator: 'hailo-8', fps: 268, watts: 1.6 },
      { name: 'scrfd_face', accelerator: 'hailo-8', fps: 114, watts: 1.0 },
      { name: 'yolov8s_pose', accelerator: 'hailo-8', fps: 228, watts: 3.3 },
    ]
  };

  return new Response(JSON.stringify(models, null, 2), {
    headers: { ...corsHeaders, 'Content-Type': 'application/json' }
  });
}

async function handleGenerate(request, corsHeaders) {
  try {
    const body = await request.json();
    const { prompt, model = 'llama3.2:1b', stream = false } = body;

    if (!prompt) {
      return new Response(JSON.stringify({ error: 'prompt required' }), {
        status: 400,
        headers: { ...corsHeaders, 'Content-Type': 'application/json' }
      });
    }

    // In production, forward to cluster via tunnel
    // For now, return a simulated response
    const response = {
      model,
      prompt,
      response: `[Simulated] This would be generated by ${model} on the BlackRoad cluster. The actual response would come from one of 4 Pi nodes running Ollama.`,
      stats: {
        node: 'cecilia',
        tokens: 42,
        time_sec: 8.5,
        tokens_per_sec: 4.9,
      }
    };

    return new Response(JSON.stringify(response, null, 2), {
      headers: { ...corsHeaders, 'Content-Type': 'application/json' }
    });
  } catch (e) {
    return new Response(JSON.stringify({ error: e.message }), {
      status: 500,
      headers: { ...corsHeaders, 'Content-Type': 'application/json' }
    });
  }
}

async function handleChat(request, corsHeaders) {
  try {
    const body = await request.json();
    const { messages, model = 'llama3.2:1b' } = body;

    if (!messages || !Array.isArray(messages)) {
      return new Response(JSON.stringify({ error: 'messages array required' }), {
        status: 400,
        headers: { ...corsHeaders, 'Content-Type': 'application/json' }
      });
    }

    // Format messages into prompt
    const prompt = messages.map(m => `${m.role}: ${m.content}`).join('\n');

    // Simulated response
    const response = {
      model,
      choices: [{
        message: {
          role: 'assistant',
          content: `[Simulated] Response from BlackRoad cluster running ${model}.`
        }
      }],
      usage: {
        prompt_tokens: 20,
        completion_tokens: 15,
        total_tokens: 35,
      }
    };

    return new Response(JSON.stringify(response, null, 2), {
      headers: { ...corsHeaders, 'Content-Type': 'application/json' }
    });
  } catch (e) {
    return new Response(JSON.stringify({ error: e.message }), {
      status: 500,
      headers: { ...corsHeaders, 'Content-Type': 'application/json' }
    });
  }
}
